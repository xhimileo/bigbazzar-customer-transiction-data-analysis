{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "products = pd.read_csv('cproducts.csv')\n",
    "tender = pd.read_csv('ctender.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are trying to cluster by Store location, we will not include the 2nd file in our clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill missing values\n",
    "products['promotion_description'].fillna('no_promo', inplace=True)\n",
    "products['Gender'].fillna('no_gender', inplace=True)\n",
    "products['State'].fillna('no_state', inplace=True)\n",
    "products['PinCode'].fillna(-1, inplace=True)\n",
    "products['DOB'].fillna(\"1\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many states are occuring multiple times due to misspelling, extra space, short form or case sensitivity. So I made this dictionary to convert them to one value per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {'MADHY PRADESH':'MADHYA PRADESH', 'TAMILNADU':'TAMIL NADU', 'MADHYA  PRADESH':'MADHYA PRADESH', 'HARAYANA':'HARYANA',\n",
    "             'Jharkhand':'JHARKHAND','Tamilnadu':'TAMIL NADU','Tamil Nadu':'TAMIL NADU','Madhya Pradesh':'MADHYA PRADESH',\n",
    "             'REST OF WEST BENGAL':'WEST BENGAL', 'west bengal':'WEST BENGAL','Uttar Pradesh':'UTTAR PRADESH', 'Delhi':'DELHI',\n",
    "             'Bhopal':'BHOPAL','CHHATISGARH':'CHHATTISGARH','CHATTISGARH':'CHHATTISGARH', 'jharkhand':'JHARKHAND','Chandigarh':'CHANDIGARH',\n",
    "             'UTTAR PRADESH WEST': 'UTTAR PRADESH','ODISHA':'ORISSA','MAHARASTRA':'MAHARASHTRA','madhya pradesh':'MADHYA PRADESH',\n",
    "             'KARNATAK':'KARNATAKA','JAMMU and KASHMIR':'JAMMU AND KASHMIR','JAMMU KASHMIR':'JAMMU AND KASHMIR','Rajasthan':'RAJASTHAN',\n",
    "             'east singhbhum':'JHARKHAND', 'ORRISA':'ORISSA','Andhra Pradesh':'ANDHRA PRADESH', 'UTTARANCHAL':'UTTARAKHAND',\n",
    "             'Uttar pradesh':'UTTAR PRADESH','Maharashtra':'MAHARASHTRA','MP':'MADHYA PRADESH', 'UTTAR PRADESH EAST':'UTTAR PRADESH',\n",
    "             'Punjab':'PUNJAB','maharashtra':'MAHARASHTRA','Karnataka':'KARNATAKA','M.P.':'MADHYA PRADESH','DAMAN':'DAMAN AND DIU',\n",
    "             'HUBLI':'KARNATAKA','Tamil nadu':'TAMIL NADU','GUJRAT':'GUJARAT', 'Mp':'MADHYA PRADESH','Madhya pradesh':'MADHYA PRADESH',\n",
    "             'West Bengal':'WEST BENGAL','Gujarat':'GUJARAT','UP':'UTTAR PRADESH','Chennai':'CHENNAI', 'm.p.':'MADHYA PRADESH',\n",
    "             'kerala':'KERALA'}\n",
    "\n",
    "products.replace({\"State\": state_dict}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert data into numeric / float\n",
    "\n",
    "for c in products.columns:\n",
    "    lbl = LabelEncoder()\n",
    "    if products[c].dtype == 'object' and c not in ['store_description','customerID','transactionDate']:\n",
    "        #products[c] = products[c].factorize()[0]\n",
    "        products[c] = lbl.fit_transform(products[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving these variables to be used in making the final submission\n",
    "customers = products['customerID']\n",
    "store_codes = products['store_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new = products.drop('customerID', axis=1, inplace=True)\n",
    "products_new = products.drop('transactionDate', axis=1, inplace=True)\n",
    "products_new = products.drop('store_description', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new = products_new['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode the states\n",
    "products_new = pd.get_dummies(products_new, columns='State', prefix='State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Selecting Samples\n",
    "\n",
    "To get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, we will add three indices of our choice to the indices list which will represent the customers to track. It is suggested to try different sets of samples until we obtain customers that vary significantly from one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select three indices of your choice you wish to sample from the dataset\n",
    "indices = [60,110,160]\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(products_new.loc[indices], columns = products_new.keys()).reset_index(drop = True)\n",
    "print (\"Chosen samples of wholesale customers dataset:\")\n",
    "display(samples)\n",
    "\n",
    "\n",
    "percentiles = products_new.rank(pct=True)\n",
    "percentiles = 100*percentiles.round(decimals=3)\n",
    "percentiles = percentiles.iloc[indices]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axes().set_title(\"HeatMap\")\n",
    "sns.heatmap(percentiles, vmin=1, vmax=99, annot=True, cbar=False, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Transformation\n",
    "\n",
    "In this section we will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.\n",
    " ## Implementation: PCA\n",
    "\n",
    "We can now apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension â€” how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n",
    "\n",
    "In the code block below, we will need to implement the following:\n",
    "\n",
    "    Import sklearn.decomposition.PCA and assign the results of fitting PCA in 6 dimensions with products_new to pca.\n",
    "    Apply a PCA transformation of samples using pca.transform, and assign the results to pca_samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components=6).fit(products_new)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(products_new)\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = pca.transform(samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "#reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "pca_results = vs.pca_results(products_new, pca)\n",
    "\n",
    "print(pca_results['Explained Variance'].cumsum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components=5).fit(products_new)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(products_new)\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = pca.transform(samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4', 'Dimension 5'])\n",
    "\n",
    "vs.pca_results(products_new, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample log-data after applying PCA transformation in two dimensions\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4', 'Dimension 5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating clusters\n",
    "\n",
    "GMM is a lot more flexible in terms of cluster covariance. K-means is actually a special case of GMM in which each clusterâ€™s covariance along all dimensions approaches 0. This implies that a point will get assigned only to the cluster closest to it. With GMM, each cluster can have unconstrained covariance structure. Think of rotated and/or elongated distribution of points in a cluster, instead of spherical as in K-means. As a result, cluster assignment is much more flexible in GMM than in K-means.\n",
    "## Implementation: Creating Clusters\n",
    "\n",
    "Depending on the problem, the number of clusters that we expect to be in the data may already be known. When the number of clusters is not known a priori, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data â€” if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's silhouette coefficient. The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the mean silhouette coefficient provides for a simple scoring method of a given clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def silhouette(k):\n",
    "    global clusterer, preds, centers, sample_preds\n",
    "    \n",
    "    # Apply your clustering algorithm of choice to the reduced data \n",
    "    clusterer = GaussianMixture(n_components=k, random_state=0)\n",
    "    clusterer.fit(reduced_data)\n",
    "\n",
    "    # Predict the cluster for each data point\n",
    "    preds = clusterer.predict(reduced_data)\n",
    "\n",
    "    # Find the cluster centers\n",
    "    centers = clusterer.means_ \n",
    "    \n",
    "    # Predict the cluster for each transformed sample data point\n",
    "    sample_preds = clusterer.predict(pca_samples)\n",
    "\n",
    "    # Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "    score = silhouette_score(reduced_data,preds)\n",
    "    return score, preds\n",
    "\n",
    "results = pd.DataFrame(columns=['Silhouette Score'])\n",
    "results.columns.name = 'Number of Clusters'    \n",
    "for k in range(2,16):\n",
    "    score,_ = silhouette(k) \n",
    "    results = results.append(pd.DataFrame([score],columns=['Silhouette Score'],index=[k]))\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cluster Visualization\n",
    "\n",
    "Once we've chosen the optimal number of clusters for our clustering algorithm using the scoring metric above, we can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters.\n",
    "\n",
    "## Cluster Visualization\n",
    "\n",
    "Once we've chosen the optimal number of clusters for our clustering algorithm using the scoring metric above, we can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimum silhouette score\n",
    "scores, preds = silhouette(7)\n",
    "\n",
    "# Display the results of the clustering from implementation\n",
    "vs.cluster_results(reduced_data, preds, centers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Data Recovery\n",
    "\n",
    "Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the averages of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to the average customer of that segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the true centers\n",
    "\n",
    "segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n",
    "true_centers = pd.DataFrame(np.round(centers), columns = reduced_data.keys())\n",
    "true_centers.index = segments\n",
    "display(true_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions\n",
    "for i, pred in enumerate(sample_preds):\n",
    "    print (\"Sample point\", i, \"predicted to be in Cluster\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create submission files\n",
    "sub2 = pd.DataFrame({'customerID':customers, 'store_code':store_codes, 'cluster':preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sub6.txt', reduced_data)\n",
    "sub2.reindex(columns=[\"customerID\",\"store_code\",\"cluster\"]).to_csv('sub6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
