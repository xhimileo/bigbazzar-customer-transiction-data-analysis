{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "products = pd.read_csv('../cproducts.csv')\n",
    "tender = pd.read_csv('../ctender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill missing values\n",
    "\n",
    "products['promotion_description'].fillna('no_promo', inplace=True)\n",
    "products['Gender'].fillna('no_gender', inplace=True)\n",
    "products['State'].fillna('no_state', inplace=True)\n",
    "products['PinCode'].fillna(-1, inplace=True)\n",
    "products['DOB'].fillna(\"1\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert data into numeric / float\n",
    "\n",
    "for c in products.columns:\n",
    "    lbl = LabelEncoder()\n",
    "    if products[c].dtype == 'object' and c not in ['store_description','customerID','transactionDate']:\n",
    "        products[c] = lbl.fit_transform(products[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = products['customerID']\n",
    "store_codes = products['store_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new = products.drop('customerID', axis=1, inplace=True)\n",
    "products_new = products.drop('transactionDate', axis=1, inplace=True)\n",
    "products_new = products.drop('store_description', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Selecting Samples¶\n",
    "\n",
    "To get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, we will add three indices of our choice to the indices list which will represent the customers to track. It is suggested to try different sets of samples until we obtain customers that vary significantly from one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select three indices of your choice you wish to sample from the dataset\n",
    "indices = [60,110,160]\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(products_new.loc[indices], columns = products_new.keys()).reset_index(drop = True)\n",
    "print (\"Chosen samples of wholesale customers dataset:\")\n",
    "display(samples)\n",
    "\n",
    "\n",
    "percentiles = products_new.rank(pct=True)\n",
    "percentiles = 100*percentiles.round(decimals=3)\n",
    "percentiles = percentiles.iloc[indices]\n",
    "\n",
    "plt.axes().set_title(\"HeatMap\")\n",
    "sns.heatmap(percentiles, vmin=1, vmax=99, annot=True, cbar=False, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Feature Relevance\n",
    "\n",
    "One interesting thought to consider is if one (or more) of the thirteen product categories is actually relevant for understanding customer purchasing. We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature. In the code block below, we will need to implement the following: Assign new_data a copy of the data by removing a feature of our choice using the DataFrame.drop function. Use sklearn.model_selection.train_test_split to split the dataset into training and testing sets. Use the removed feature as our target label. Set a test_size of 0.25 and set a random_state. Import a decision tree regressor, set a random_state, and fit the learner to the training data. Report the prediction score of the testing set using the regressor's score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# TODO: Make a copy of the DataFrame, using the 'drop' function to drop the given feature\n",
    "store_data = products_new['store_code']\n",
    "new_data = products_new.drop('store_code', axis=1)\n",
    "\n",
    "# TODO: Split the data into training and testing sets using the given feature as the target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(new_data, store_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# TODO: Create a decision tree regressor and fit it to the training set\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "pred = regressor.predict(X_test)\n",
    "# TODO: Report the score of the prediction using the testing set\n",
    "score = r2_score(y_test,pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reported coefficient of determination or the R squared value of store_code was 0.75 which shows a strong fit of the model to the data. This feature may not be necessary to identify customers' spending habit as this can be pretty easily deduced from the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a scatter matrix for each pair of features in the data\n",
    "pd.plotting.scatter_matrix(products_new, alpha = 0.5, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Transformation\n",
    "\n",
    "In this section we will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.\n",
    "\n",
    "## Implementation: PCA\n",
    "\n",
    "We can now apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension — how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n",
    "\n",
    "In the code block below, we will need to implement the following:\n",
    "\n",
    "    Import sklearn.decomposition.PCA and assign the results of fitting PCA in 6 dimensions with products_new to pca.\n",
    "    Apply a PCA transformation of samples using pca.transform, and assign the results to pca_samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components=6).fit(products_new)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(products_new)\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = pca.transform(samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "#reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "pca_results = vs.pca_results(products_new, pca)\n",
    "\n",
    "print(pca_results['Explained Variance'].cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components=2).fit(products_new)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(products_new)\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = pca.transform(samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "vs.pca_results(products_new, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample log-data after applying PCA transformation in two dimensions\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating clusters\n",
    "\n",
    "GMM is a lot more flexible in terms of cluster covariance. K-means is actually a special case of GMM in which each cluster’s covariance along all dimensions approaches 0. This implies that a point will get assigned only to the cluster closest to it. With GMM, each cluster can have unconstrained covariance structure. Think of rotated and/or elongated distribution of points in a cluster, instead of spherical as in K-means. As a result, cluster assignment is much more flexible in GMM than in K-means.\n",
    "## Implementation: Creating Clusters\n",
    "\n",
    "Depending on the problem, the number of clusters that we expect to be in the data may already be known. When the number of clusters is not known a priori, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data — if any. However, we can quantify the \"goodness\" of a clustering by calculating each data point's silhouette coefficient. The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the mean silhouette coefficient provides for a simple scoring method of a given clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def silhouette(k):\n",
    "    global clusterer, preds, centers, sample_preds\n",
    "    \n",
    "    # Apply your clustering algorithm of choice to the reduced data \n",
    "    clusterer = GaussianMixture(n_components=k, random_state=0)\n",
    "    clusterer.fit(reduced_data)\n",
    "\n",
    "    # Predict the cluster for each data point\n",
    "    preds = clusterer.predict(reduced_data)\n",
    "\n",
    "    # Find the cluster centers\n",
    "    centers = clusterer.means_ \n",
    "    \n",
    "    # Predict the cluster for each transformed sample data point\n",
    "    sample_preds = clusterer.predict(pca_samples)\n",
    "\n",
    "    # Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "    score = silhouette_score(reduced_data,preds)\n",
    "    return score, preds\n",
    "\n",
    "results = pd.DataFrame(columns=['Silhouette Score'])\n",
    "results.columns.name = 'Number of Clusters'    \n",
    "for k in range(2,16):\n",
    "    score,_ = silhouette(k) \n",
    "    results = results.append(pd.DataFrame([score],columns=['Silhouette Score'],index=[k]))\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cluster Visualization\n",
    "\n",
    "Once we've chosen the optimal number of clusters for our clustering algorithm using the scoring metric above, we can now visualize the results by executing the code block below. Note that, for experimentation purposes, you are welcome to adjust the number of clusters for your clustering algorithm to see various visualizations. The final visualization provided should, however, correspond with the optimal number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimum silhouette score\n",
    "scores, preds = silhouette(5)\n",
    "\n",
    "# Display the results of the clustering from implementation\n",
    "vs.cluster_results(reduced_data, preds, centers, pca_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation: Data Recovery\n",
    "\n",
    "Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the averages of all the data points predicted in the respective clusters. For the problem of creating customer segments, a cluster's center point corresponds to the average customer of that segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the true centers\n",
    "segments = ['Segment {}'.format(i) for i in range(0,len(centers))]\n",
    "true_centers = pd.DataFrame(np.round(centers), columns = reduced_data.keys())\n",
    "true_centers.index = segments\n",
    "display(true_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions\n",
    "for i, pred in enumerate(sample_preds):\n",
    "    print (\"Sample point\", i, \"predicted to be in Cluster\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create submission files\n",
    "sub2 = pd.DataFrame({'customerID':customers, 'store_code':store_codes, 'cluster':preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../subthree_new.txt', reduced_data)\n",
    "sub2.to_csv('../subthree_new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
