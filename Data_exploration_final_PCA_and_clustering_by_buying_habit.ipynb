{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "train = pd.read_csv('products_transactional.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill missing values\n",
    "#products['customerID'].fillna('BBID_0000', inplace=True)\n",
    "train['promotion_description'].fillna('no_promo', inplace=True)\n",
    "train['Gender'].fillna('no_gender', inplace=True)\n",
    "train['State'].fillna('no_state', inplace=True)\n",
    "train['PinCode'].fillna(-1, inplace=True)\n",
    "train['DOB'].fillna(\"1\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing data for product_code\n",
    "train = train[np.isfinite(train['product_code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting bad states\n",
    "state_dict = {'MADHY PRADESH':'MADHYA PRADESH', 'TAMILNADU':'TAMIL NADU', 'MADHYA  PRADESH':'MADHYA PRADESH', 'HARAYANA':'HARYANA',\n",
    "             'Jharkhand':'JHARKHAND','Tamilnadu':'TAMIL NADU','Tamil Nadu':'TAMIL NADU','Madhya Pradesh':'MADHYA PRADESH',\n",
    "             'REST OF WEST BENGAL':'WEST BENGAL', 'west bengal':'WEST BENGAL','Uttar Pradesh':'UTTAR PRADESH', 'Delhi':'DELHI',\n",
    "             'Bhopal':'BHOPAL','CHHATISGARH':'CHHATTISGARH','CHATTISGARH':'CHHATTISGARH', 'jharkhand':'JHARKHAND','Chandigarh':'CHANDIGARH',\n",
    "             'UTTAR PRADESH WEST': 'UTTAR PRADESH','ODISHA':'ORISSA','MAHARASTRA':'MAHARASHTRA','madhya pradesh':'MADHYA PRADESH',\n",
    "             'KARNATAK':'KARNATAKA','JAMMU and KASHMIR':'JAMMU AND KASHMIR','JAMMU KASHMIR':'JAMMU AND KASHMIR','Rajasthan':'RAJASTHAN',\n",
    "             'east singhbhum':'JHARKHAND', 'ORRISA':'ORISSA','Andhra Pradesh':'ANDHRA PRADESH', 'UTTARANCHAL':'UTTARAKHAND',\n",
    "             'Uttar pradesh':'UTTAR PRADESH','Maharashtra':'MAHARASHTRA','MP':'MADHYA PRADESH', 'UTTAR PRADESH EAST':'UTTAR PRADESH',\n",
    "             'Punjab':'PUNJAB','maharashtra':'MAHARASHTRA','Karnataka':'KARNATAKA','M.P.':'MADHYA PRADESH','DAMAN':'DAMAN AND DIU',\n",
    "             'HUBLI':'KARNATAKA','Tamil nadu':'TAMIL NADU','GUJRAT':'GUJARAT', 'Mp':'MADHYA PRADESH','Madhya pradesh':'MADHYA PRADESH',\n",
    "             'West Bengal':'WEST BENGAL','Gujarat':'GUJARAT','UP':'UTTAR PRADESH','Chennai':'CHENNAI', 'm.p.':'MADHYA PRADESH',\n",
    "             'kerala':'KERALA'}\n",
    "\n",
    "train.replace({\"State\": state_dict}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"transactionDate\"] = pd.to_datetime(train[\"transactionDate\"],format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"month\"] = pd.DatetimeIndex(train[\"transactionDate\"]).month\n",
    "train[\"day\"] = pd.DatetimeIndex(train[\"transactionDate\"]).day\n",
    "train[\"dayofweek\"] = pd.DatetimeIndex(train[\"transactionDate\"]).dayofweek\n",
    "train[\"year\"] = pd.DatetimeIndex(train[\"transactionDate\"]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of order by week day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"dayofweek\", data=train, color=color[0])\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Day of week', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Frequency of order by week day\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the transactions are highest on Sunday, Wednesday and Saturday\n",
    "\n",
    "\n",
    "## Frequency of order by day in a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"day\", data=train, color=color[0])\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Day of Month', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Frequency of order by days in a month\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of order by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"month\", data=train, color=color[0])\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Frequency of order by month\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the sales are highest around May-June\n",
    "\n",
    "\n",
    "## Frequency of Orde by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"year\", data=train, color=color[0])\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Frequency of order by Year\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016 had the highest sales although for 2017, we only have data till July.\n",
    "\n",
    "\n",
    "## Heatmap of Month Vs Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = train.groupby([\"month\", \"day\"])[\"product_code\"].aggregate(\"count\").reset_index()\n",
    "grouped_df = grouped_df.pivot('month', 'day', 'product_code')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(grouped_df)\n",
    "plt.title(\"HeatMap of amount of sales on Month vs Day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 20 most popular products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = train['product_description'].value_counts().reset_index().head(20)\n",
    "cnt_srs.columns = ['product_name', 'frequency_count']\n",
    "cnt_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = train['product_description'].value_counts().head(20)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[5])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Product', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions by State(Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "temp_series = train['State'].value_counts().head(10)\n",
    "labels = (np.array(temp_series.index))\n",
    "sizes = (np.array((temp_series / temp_series.sum())*100))\n",
    "plt.pie(sizes, labels=labels, \n",
    "        autopct='%1.1f%%', startangle=200)\n",
    "plt.title(\"State distribution\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product analysis by TFIDF\n",
    "\n",
    "Now we will see if the model will learn any useful information about the products from the order history of all users, maybe in the future this can be used as input to a classifier that recommends products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"product_description\"] = train[\"product_description\"].astype(str)\n",
    "train_tfidf = train.groupby(\"customerID\").apply(lambda order: order['product_description'].tolist())\n",
    "train_tfidf = train_tfidf.reset_index()\n",
    "train_tfidf.columns = ['customerID','product_set']\n",
    "train_tfidf.product_set = train_tfidf.product_set.astype(str)\n",
    "train_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=5, max_features=1000\n",
    "                        , strip_accents='unicode',lowercase =True,\n",
    "analyzer='word', token_pattern=r'\\w+', use_idf=True, \n",
    "smooth_idf=True, sublinear_tf=True, stop_words = 'english')\n",
    "tfidf.fit(train_tfidf['product_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "text = tfidf.transform(train_tfidf['product_set'])\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "text = svd.fit_transform(text)\n",
    "text = pd.DataFrame(text)\n",
    "text.columns = ['pf_0','pf_1']\n",
    "text['customerID'] = train_tfidf.customerID\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.plot(text['pf_0'].head(50),text['pf_1'].head(50),'r*',label=text['customerID'].head(50))\n",
    "for row in text.head(50).itertuples():\n",
    "    plt.annotate('user_'+str(row.customerID), xy=(row.pf_0,row.pf_1), \n",
    "            xytext=(row.pf_0+0.01,row.pf_1+0.01)\n",
    "            \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = train['product_description'].value_counts().reset_index().head(500)\n",
    "cnt_srs.columns = ['product_name', 'frequency_count']\n",
    "cnt_srs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the top 5 products are: BB-CB-27X30X208SWG NEW, BB-CB-20X25X208SWG NEW, SUGAR MEDIUM LOOSE, BB-CB-20X25X168SWG-Suitable for ROI New and TOMATO LOOSE. We will confirm this hypothesis later with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_500 = train[train['product_description'].isin(cnt_srs.product_name)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_top_500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crosstabbing cust_id and top 20 prods\n",
    "cust_prod = pd.crosstab(train_top_500['customerID'], train_top_500['product_description'])\n",
    "cust_prod.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have selected 6 dimensions for the PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(cust_prod)\n",
    "pca_samples = pca.transform(cust_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = pd.DataFrame(pca_samples)\n",
    "ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "tocluster = pd.DataFrame(ps[[4,1]])\n",
    "print (tocluster.shape)\n",
    "print (tocluster.head())\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(tocluster[4], tocluster[1], 'o', markersize=2, color='blue', alpha=0.5, label='class1')\n",
    "\n",
    "plt.xlabel('x_values')\n",
    "plt.ylabel('y_values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "clusterer = KMeans(n_clusters=4,random_state=42).fit(tocluster)\n",
    "centers = clusterer.cluster_centers_\n",
    "c_preds = clusterer.predict(tocluster)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print first 200 predictions\n",
    "print (c_preds[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "colors = ['orange','blue','purple','green']\n",
    "colored = [colors[k] for k in c_preds]\n",
    "print (colored[0:10])\n",
    "plt.scatter(tocluster[4],tocluster[1],  color = colored)\n",
    "for ci,c in enumerate(centers):\n",
    "    plt.plot(c[0], c[1], 'o', markersize=8, color='red', alpha=0.9, label=''+str(ci))\n",
    "\n",
    "plt.xlabel('x_values')\n",
    "plt.ylabel('y_values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So now that we have a possible clustering for our customers, let's see if there are any more interesting patterns beneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the cluster prediction column to the dataframe\n",
    "clust_prod = cust_prod.copy()\n",
    "clust_prod['cluster'] = c_preds\n",
    "\n",
    "clust_prod.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (clust_prod.shape)\n",
    "f,arr = plt.subplots(2,2,sharex=True,figsize=(15,15))\n",
    "\n",
    "c1_count = len(clust_prod[clust_prod['cluster']==0])\n",
    "\n",
    "#Find the mean for each cluster and plot it against the top 500 products\n",
    "c0 = clust_prod[clust_prod['cluster']==0].drop('cluster',axis=1).mean()\n",
    "arr[0,0].bar(range(len(clust_prod.drop('cluster',axis=1).columns)),c0)\n",
    "c1 = clust_prod[clust_prod['cluster']==1].drop('cluster',axis=1).mean()\n",
    "arr[0,1].bar(range(len(clust_prod.drop('cluster',axis=1).columns)),c1)\n",
    "c2 = clust_prod[clust_prod['cluster']==2].drop('cluster',axis=1).mean()\n",
    "arr[1,0].bar(range(len(clust_prod.drop('cluster',axis=1).columns)),c2)\n",
    "c3 = clust_prod[clust_prod['cluster']==3].drop('cluster',axis=1).mean()\n",
    "arr[1,1].bar(range(len(clust_prod.drop('cluster',axis=1).columns)),c3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what are the top 10 goods bought by people of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster c0\n",
    "c0.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster c1\n",
    "c1.sort_values(ascending=False)[0:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster c2\n",
    "c2.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster c3\n",
    "c3.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing here is even though the 4 products: BB-CB-27X30X208SWG NEW, BB-CB-20X25X208SWG NEW, SUGAR MEDIUM LOOSE, BB-CB-20X25X168SWG-Suitable for ROI New confirm our hypothesis which we made previously about the top 5 products, TOMATO LOOSE does not fall under this category and is only present in one of the clusters.\n",
    "\n",
    "What we can inspect here is if clusters differ in quantities and proportions, with respect of these goods, or if a cluster is characterized by some goods not included in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "cluster_means = [[c0['BB-CB-27X30X208SWG NEW'],c0['BB-CB-20X25X208SWG NEW'],c0['SUGAR MEDIUM LOOSE'], c0['BB-CB-20X25X168SWG-Suitable for ROI New'], c0['TOMATO LOOSE']],\n",
    "                 [c1['BB-CB-27X30X208SWG NEW'],c1['BB-CB-20X25X208SWG NEW'],c1['SUGAR MEDIUM LOOSE'], c1['BB-CB-20X25X168SWG-Suitable for ROI New'], c1['TOMATO LOOSE']],\n",
    "                 [c2['BB-CB-27X30X208SWG NEW'],c2['BB-CB-20X25X208SWG NEW'],c2['SUGAR MEDIUM LOOSE'], c2['BB-CB-20X25X168SWG-Suitable for ROI New'], c2['TOMATO LOOSE']],\n",
    "                 [c3['BB-CB-27X30X208SWG NEW'],c3['BB-CB-20X25X208SWG NEW'],c3['SUGAR MEDIUM LOOSE'], c3['BB-CB-20X25X168SWG-Suitable for ROI New'], c3['TOMATO LOOSE']]]\n",
    "cluster_means = pd.DataFrame(cluster_means, columns = ['BB-CB-27X30X208SWG NEW','BB-CB-20X25X208SWG NEW','SUGAR MEDIUM LOOSE','BB-CB-20X25X168SWG-Suitable for ROI New','TOMATO LOOSE'])\n",
    "HTML(cluster_means.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table depicts the percentage these goods with respect to the other top 5 in each cluster.\n",
    "\n",
    "It seems people of cluster 3 buy much more sugar than people from other clusters.\n",
    "\n",
    "Another interesting observation is that people in cluster 2 and 3 buy a huge lot of TUR DAL even though it is not even there in the top 10 products. So this is where the unique properties of the clusters really shows up and this can be really used to a great use in later product recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
